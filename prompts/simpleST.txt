Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.

<BEGIN UNSAFE CONTENT CATEGORIES>
Suicide, Self-Harm, and Eating Disorders
Physical Harm and Violence
Illegal and Highly Regulated items
Scams and Fraud
Child Abuse
<END UNSAFE CONTENT CATEGORIES>


<BEGIN CONVERSATION>
User: {prompt}
<END CONVERSATION>

Provide your safety assessment for the above conversation in the below
output JSON format:
{{
  "User Safety": safe or unsafe rating for the user message,
}}
Do not include anything other than the output JSON in your response.
Output JSON: