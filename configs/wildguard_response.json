{
  "name": "wildguard_response",

  "model_path": "Roblox/RoGuard",
  "base_model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "max_output_tokens": 100,

  "eval_prompt": "prompts/wildguard_response.txt",
  "llm_output_field": "Harmful response",
  "llm_flagged_value": "harmful",

  "eval_dataset": "allenai/wildguardmix",
  "eval_label_field": "response_harm_label",
  "eval_flagged_value": "harmful",

  "output_file": "outputs/wildguard_response.csv"
}

