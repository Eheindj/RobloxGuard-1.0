{
  "name": "harmbench",

  "model_path": "Roblox/RoGuard-1.0",
  "base_model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "max_output_tokens": 100,

  "eval_prompt": "prompts/harmbench.txt",
  "llm_output_field": "Response Safety",
  "llm_flagged_value": "unsafe",

  "eval_dataset": "datasets/text_behaviors_val_set.json",
  "eval_label_field": "violation",
  "eval_flagged_value": "1",

  "output_file": "outputs/harmbench.csv"
}
